{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from gym.envs.toy_text.frozen_lake import LEFT, RIGHT, DOWN, UP\n",
    "from gym.envs.toy_text import frozen_lake, discrete\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='D4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False})\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    env: gym.core.Environment\n",
    "    Environment to play on.\n",
    "\n",
    "    env.P: dictionary\n",
    "    It is from gym.core.Environment\n",
    "    P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    \n",
    "    env.nS: int\n",
    "    count of states \n",
    "    \n",
    "    env.nA: int\n",
    "    count of actions available\n",
    "    \n",
    "    action_space: discrete\n",
    "                LEFT = 0\n",
    "                DOWN = 1\n",
    "                RIGHT = 2\n",
    "                UP = 3\n",
    "    ENVIRONMENT: \n",
    "                \"SFFF\",\n",
    "                \"FHFH\",\n",
    "                \"FFFH\",\n",
    "                \"HFFG\"\n",
    "\"\"\"\n",
    "\n",
    "def print_policy(policy, action_names):\n",
    "    \"\"\" \n",
    "    Print the policy in human-readable format.\n",
    "    \"\"\"\n",
    "    str_policy = policy.astype('str')\n",
    "    for action_num, action_name in action_names.items():\n",
    "        np.place(str_policy, policy == action_num, action_name)\n",
    "\n",
    "    print(str_policy[0:4])\n",
    "    print(str_policy[4:8])\n",
    "    print(str_policy[8:12])\n",
    "    print(str_policy[12:16])\n",
    "    \n",
    "    return str_policy\n",
    "\n",
    "action_names = {LEFT: 'LEFT', RIGHT: 'RIGHT', DOWN: 'DOWN', UP: 'UP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('D4x4-FrozenLake-v0')\n",
    "grid = 4\n",
    "gamma = 0.9 # Change this to play with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Value iteration ##############################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"\n",
    "    Runs value iteration for a given gamma and environment. Return \n",
    "    the value function and the number of iterations it took to converge.\n",
    "        See section 4.4 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "    value_func_old = np.random.rand(env.nS)\n",
    "    value_func_new = np.zeros(env.nS)\n",
    "    value_func_collect = np.zeros((env.nS,max_iterations))\n",
    "    delta_collect = np.zeros(max_iterations)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        delta=0\n",
    "        for s in range(env.nS):\n",
    "            max_value_func = -1\n",
    "            \n",
    "            # Refer Figure 4.5 for the algorithm\n",
    "            # Parse through env.nA for finding the maximum\n",
    "            # Using env.P[s][a] implement the V(s) updation\n",
    "                                \n",
    "            # Enter your code here\n",
    "            \n",
    "            ## Hint: Start with using for loops accessing \n",
    "            ## the actions and next states \n",
    "            ## via env.nA and env.P[s][a] respectively\n",
    "            \n",
    "            # Few code compatibility instructions:\n",
    "            ### Updated V(s) must be in variable \"max_value_func\"\n",
    "            ### The older Value function V(s) is accessed by the variable \"value_func_old\"\n",
    "                    \n",
    "            diff=abs(value_func_old[s]-max_value_func)\n",
    "            delta=max(delta,diff)\n",
    "            value_func_new[s]=max_value_func\n",
    "\n",
    "        delta_collect[iteration] = delta\n",
    "        value_func_old = value_func_new\n",
    "        value_func_collect[:,iteration] = value_func_old\n",
    "        # Stopping criteria: STOP when the sup norm of (V_k-V_{k-1}) is less than some tolerance level(it's 1e-3 here).\n",
    "        if delta<=tol: break\n",
    "\n",
    "    temp=np.repeat(value_func_collect[:,iteration], iteration, axis=0)\n",
    "    temp=temp.reshape(16,iteration)\n",
    "    temp1 = abs(value_func_collect[:,0:iteration]-temp);\n",
    "    distance_from_converged_value_func = temp1.max(axis=0)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"Value iteration convergence plots\", fontsize=16)\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    ax.plot(range(1,iteration+1),delta_collect[0:iteration])\n",
    "    ax.set_title('||V_k-V_{k-1}|| plot')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "\n",
    "    ax = plt.subplot(2,1,2)\n",
    "    ax.plot(range(1,iteration+1),distance_from_converged_value_func)\n",
    "    ax.set_title('||V_k-V^*|| plot')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    return value_func_new, iteration\n",
    "\n",
    "\n",
    "def value_function_to_policy(env, gamma, value_function):\n",
    "    \"\"\"\n",
    "    Mapping actions for each state using the value_function to get a policy\n",
    "        See section 4.4 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "    policy=np.zeros(env.nS,dtype='int')\n",
    "    for s in range(env.nS):\n",
    "        max_value_func=-1\n",
    "        max_action=-1\n",
    "        \n",
    "        # Copy and paste the V(s) updation which you've implemented in the \n",
    "        # value_iteration function here. Required changes explained further:\n",
    "        \n",
    "        # In addition to what you did before, \n",
    "        # return action which is the best (according to maximum V(s))\n",
    "        # assign that action to the variable \"max_action\"\n",
    "        \n",
    "        policy[s]=max_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "print(\"\\nDoing Value Iteration\")\n",
    "start_time=time.time()\n",
    "value_function,value_iters=value_iteration(env,gamma)\n",
    "print(\"Total time taken: \"+str((time.time()-start_time)))\n",
    "print(\"Total Value Iteration Steps: \"+str(value_iters))\n",
    "print(\"Policy:\")\n",
    "policy=value_function_to_policy(env,gamma,value_function)\n",
    "policy_str=print_policy(policy,action_names)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = seaborn.diverging_palette(220, 10, as_cmap=True)\n",
    "reshaped=np.reshape(value_function,(grid,grid))\n",
    "seaborn.heatmap(reshaped, cmap=cmap, vmax=1.1,\n",
    "            square=True, xticklabels=grid+1, yticklabels=grid+1,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, annot=True, fmt=\"f\")\n",
    "counter=0\n",
    "for j in range(0, 4):\n",
    "    for i in range(0, 4):\n",
    "        if policy_str[counter]==\"DOWN\":\n",
    "            plt.text(i+0.5, j+0.7, u'\\u2193', fontsize=12)\n",
    "        elif policy_str[counter]==\"UP\":\n",
    "            plt.text(i+0.5, j+0.7, u'\\u2191', fontsize=12)\n",
    "        elif policy_str[counter]==\"LEFT\":\n",
    "            plt.text(i+0.5, j+0.7, u'\\u2190', fontsize=12)\n",
    "        else:\n",
    "            plt.text(i+0.5, j+0.7, u'\\u2192', fontsize=12)\n",
    "        counter=counter+1\n",
    "\n",
    "plt.title('Heatmap of policy iteration with value function values and directions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What do you infer from the convergence plots?\n",
    "2. What do the numbers in the heatmap mean? (Hint: Check the code)\n",
    "3. Comment on comparison of these with your answers for Policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#################### Final policy animation ############################\n",
    "########################################################################\n",
    "\n",
    "flag=input(\"\\nEnter 'Y' if you want to see the final animation of the policy achieved. Else enter something else.\\n\")\n",
    "if flag==\"Y\" or flag==\"y\": print(\"Final Policy Animation\")\n",
    "def run_policy(env,gamma,policy):\n",
    "    initial_state = env.reset()\n",
    "    env.render()\n",
    "    current_state = initial_state\n",
    "    while True:\n",
    "        nextstate, reward, done, debug_info = env.step(policy[current_state])\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        current_state=nextstate\n",
    "        time.sleep(1)\n",
    "\n",
    "if flag==\"Y\" or flag==\"y\": run_policy(env,gamma,policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
