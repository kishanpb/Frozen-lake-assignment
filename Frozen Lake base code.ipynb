{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from gym.envs.toy_text.frozen_lake import LEFT, RIGHT, DOWN, UP\n",
    "from gym.envs.toy_text import frozen_lake, discrete\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='D4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False}) # Note: You have to solve by changing this bool to True.\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    env: gym.core.Environment\n",
    "    Environment to play on.\n",
    "\n",
    "    env.P: dictionary\n",
    "    It is from gym.core.Environment\n",
    "    P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    \n",
    "    env.nS: int\n",
    "    count of states \n",
    "    \n",
    "    env.nA: int\n",
    "    count of actions available\n",
    "    \n",
    "    action_space: discrete\n",
    "                LEFT = 0\n",
    "                DOWN = 1\n",
    "                RIGHT = 2\n",
    "                UP = 3\n",
    "    ENVIRONMENT: \n",
    "                \"SFFF\",\n",
    "                \"FHFH\",\n",
    "                \"FFFH\",\n",
    "                \"HFFG\"\n",
    "\"\"\"\n",
    "\n",
    "def print_policy(policy, action_names):\n",
    "    \"\"\" \n",
    "    Print and return the policy in human-readable format.\n",
    "    \"\"\"\n",
    "    str_policy = policy.astype('str')\n",
    "    for action_num, action_name in action_names.items():\n",
    "        np.place(str_policy, policy == action_num, action_name)\n",
    "    \n",
    "    print(str_policy[0:4])\n",
    "    print(str_policy[4:8])\n",
    "    print(str_policy[8:12])\n",
    "    print(str_policy[12:16])\n",
    "    \n",
    "    return str_policy\n",
    "\n",
    "action_names = {LEFT: 'LEFT', RIGHT: 'RIGHT', DOWN: 'DOWN', UP: 'UP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('D4x4-FrozenLake-v0')\n",
    "grid = 4\n",
    "gamma = 0.9 # Change this to play with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_visual(value_func,policy_str):\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = seaborn.diverging_palette(220, 10, as_cmap=True)\n",
    "    reshaped=np.reshape(value_func,(grid,grid))\n",
    "    seaborn.heatmap(reshaped, cmap=cmap, vmax=1.1,\n",
    "                square=True, xticklabels=grid+1, yticklabels=grid+1,\n",
    "                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, annot=True, fmt=\"f\")\n",
    "    counter=0\n",
    "    for j in range(0, 4):\n",
    "        for i in range(0, 4):\n",
    "            if policy_str[counter]==\"DOWN\":\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2193', fontsize=12)\n",
    "            elif policy_str[counter]==\"UP\":\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2191', fontsize=12)\n",
    "            elif policy_str[counter]==\"LEFT\":\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2190', fontsize=12)\n",
    "            else:\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2192', fontsize=12)\n",
    "            counter=counter+1\n",
    "\n",
    "    plt.title('Heatmap of policy iteration with value function values and directions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Policy iteration #############################\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def policy_evaluation(env, gamma, policy, value_func_old, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"\n",
    "        Evaluate the value of a policy.\n",
    "        See section 4.1 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def policy_improvement(env, gamma, value_func, policy):\n",
    "    \"\"\"\n",
    "      Given a policy and value function, improve the policy.\n",
    "      Returns true if policy is unchanged. Also returns the new policy.\n",
    "      See section 4.2 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"\n",
    "       Runs policy iteration.\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "       See section 4.3 of Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning) by Sutton and Barto\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint: Plot ||V_{\\pi_k}-V_{\\pi_{k-1}}|| here!\n",
    "\n",
    "    # Return these parameters:\n",
    "    # Optimal Policy, corresponding value function values. Remember, there are 16 values.\n",
    "    return policy, value_func\n",
    "\n",
    "\n",
    "print(\"Doing Policy Iteration\")\n",
    "start_time=time.time()\n",
    "policy, value_func=policy_iteration(env,gamma)\n",
    "print(\"Total time taken: \"+str((time.time()-start_time)))\n",
    "print(\"Policy:\")\n",
    "policy_str=print_policy(policy,action_names) # Prints and gets the policy in Human readable format\n",
    "fancy_visual(value_func,policy_str) # Takes in Optimal Policy and corresponding value function values to plot the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What do you infer from the convergence plots?\n",
    "2. What do the numbers in the heatmap mean? (Hint: Check the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#################### Final policy animation ############################\n",
    "########################################################################\n",
    "\n",
    "flag=input(\"\\nEnter 'Y' if you want to see the final animation of the policy achieved. Else enter something else.\\n\")\n",
    "if flag==\"Y\" or flag==\"y\": print(\"Final Policy Animation\")\n",
    "def run_policy(env,gamma,policy):\n",
    "    initial_state = env.reset()\n",
    "    env.render()\n",
    "    current_state = initial_state\n",
    "    while True:\n",
    "        nextstate, reward, done, debug_info = env.step(policy[current_state])\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        current_state=nextstate\n",
    "        time.sleep(1)\n",
    "\n",
    "if flag==\"Y\" or flag==\"y\": run_policy(env,gamma,policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
